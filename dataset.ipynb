{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scale_utils.data import Dataset\n",
    "\n",
    "# Dataset\n",
    "train_file='datasets/yahoo/yahoo-train.hdf5'\n",
    "val_file='datasets/yahoo/yahoo-val.hdf5'\n",
    "test_file='datasets/yahoo/yahoo-test.hdf5'\n",
    "train_data=Dataset(train_file)\n",
    "val_data=Dataset(val_file)\n",
    "vocab_size=train_data.vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "output_file = 'datasets_20percent/yahoo/yahoo-train-20percent.hdf5'\n",
    "\n",
    "num_batches=train_data.num_batches\n",
    "split_percentage=0.2\n",
    "\n",
    "\n",
    "split_size = int(num_batches * split_percentage)\n",
    "# Create a subset of the dataset containing 20% of the batches\n",
    "subset_indices = range(split_size)\n",
    "\n",
    "# Extract relevant data for the subset\n",
    "sents_subset = train_data.sents[:sum(train_data.batch_size[:split_size])]\n",
    "sent_lengths_subset = train_data.sent_lengths[:split_size]\n",
    "batch_size_subset = train_data.batch_size[:split_size]\n",
    "batch_idx_subset = train_data.batch_idx[:split_size]\n",
    "vocab_size = train_data.vocab_size\n",
    "\n",
    "# Create a new HDF5 file and save the subset data\n",
    "with h5py.File(output_file, 'w') as hf:\n",
    "    hf.create_dataset('source', data=sents_subset.numpy(), dtype='int64')\n",
    "    hf.create_dataset('source_l', data=sent_lengths_subset.numpy(), dtype='int64')\n",
    "    hf.create_dataset('batch_l', data=batch_size_subset.numpy(), dtype='int64')\n",
    "    hf.create_dataset('batch_idx', data=batch_idx_subset.numpy(), dtype='int64')\n",
    "    hf.create_dataset('vocab_size', data=np.array([vocab_size]), dtype='int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "output_file = 'datasets_20percent/yahoo/yahoo-val-20percent.hdf5'\n",
    "\n",
    "num_batches=val_data.num_batches\n",
    "split_percentage=0.2\n",
    "\n",
    "\n",
    "split_size = int(num_batches * split_percentage)\n",
    "# Create a subset of the dataset containing 20% of the batches\n",
    "subset_indices = range(split_size)\n",
    "\n",
    "# Extract relevant data for the subset\n",
    "sents_subset = val_data.sents[:sum(val_data.batch_size[:split_size])]\n",
    "sent_lengths_subset = val_data.sent_lengths[:split_size]\n",
    "batch_size_subset = val_data.batch_size[:split_size]\n",
    "batch_idx_subset = val_data.batch_idx[:split_size]\n",
    "vocab_size = val_data.vocab_size\n",
    "\n",
    "# Create a new HDF5 file and save the subset data\n",
    "with h5py.File(output_file, 'w') as hf:\n",
    "    hf.create_dataset('source', data=sents_subset.numpy(), dtype='int64')\n",
    "    hf.create_dataset('source_l', data=sent_lengths_subset.numpy(), dtype='int64')\n",
    "    hf.create_dataset('batch_l', data=batch_size_subset.numpy(), dtype='int64')\n",
    "    hf.create_dataset('batch_idx', data=batch_idx_subset.numpy(), dtype='int64')\n",
    "    hf.create_dataset('vocab_size', data=np.array([vocab_size]), dtype='int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
